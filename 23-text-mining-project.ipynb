{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import copy\n",
    "from nltk.stem import wordnet, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.observer import JSONLogger\n",
    "from bayes_opt.event import Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ DATASET AND CREATE TEXT AND TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Morgan Freeman 'Devastated' That Sexual Harass...</td>\n",
       "      <td>\"It is not right to equate horrific incidents ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Donald Trump Is Lovin' New McDonald's Jingle I...</td>\n",
       "      <td>It's catchy, all right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>What To Watch On Amazon Prime That’s New This ...</td>\n",
       "      <td>There's a great mini-series joining this week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Mike Myers Reveals He'd 'Like To' Do A Fourth ...</td>\n",
       "      <td>Myer's kids may be pushing for a new \"Powers\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>What To Watch On Hulu That’s New This Week</td>\n",
       "      <td>You're getting a recent Academy Award-winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "5  ENTERTAINMENT  Morgan Freeman 'Devastated' That Sexual Harass...   \n",
       "6  ENTERTAINMENT  Donald Trump Is Lovin' New McDonald's Jingle I...   \n",
       "7  ENTERTAINMENT  What To Watch On Amazon Prime That’s New This ...   \n",
       "8  ENTERTAINMENT  Mike Myers Reveals He'd 'Like To' Do A Fourth ...   \n",
       "9  ENTERTAINMENT         What To Watch On Hulu That’s New This Week   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  \n",
       "5  \"It is not right to equate horrific incidents ...  \n",
       "6                            It's catchy, all right.  \n",
       "7     There's a great mini-series joining this week.  \n",
       "8  Myer's kids may be pushing for a new \"Powers\" ...  \n",
       "9  You're getting a recent Academy Award-winning ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_json('data/News_Category_Dataset_v2.json', lines=True)\n",
    "\n",
    "# keep only text and target category\n",
    "dataset = dataset.drop(['date', 'link', 'authors'], axis=1)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['category'] = dataset['category'].map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\n",
    "dataset['category'] = dataset['category'].map(lambda x: \"ARTS & CULTURE\" if x in [\"ARTS\", \"CULTURE & ARTS\"] else x)\n",
    "dataset['category'] = dataset['category'].map(lambda x: \"STYLE & BEAUTY\" if x == \"STYLE\" else x)\n",
    "dataset['category'] = dataset['category'].map(lambda x: \"PARENTING\" if x == \"PARENTS\" else x)\n",
    "dataset['category'] = dataset['category'].map(lambda x: \"FOOD & DRINK\" if x == \"TASTE\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          32739\n",
       "WELLNESS          17827\n",
       "ENTERTAINMENT     16058\n",
       "PARENTING         12632\n",
       "STYLE & BEAUTY    11903\n",
       "TRAVEL             9887\n",
       "FOOD & DRINK       8322\n",
       "HEALTHY LIVING     6694\n",
       "QUEER VOICES       6314\n",
       "WORLDPOST          6243\n",
       "BUSINESS           5937\n",
       "COMEDY             5175\n",
       "SPORTS             4884\n",
       "BLACK VOICES       4528\n",
       "HOME & LIVING      4195\n",
       "ARTS & CULTURE     3878\n",
       "WEDDINGS           3651\n",
       "WOMEN              3490\n",
       "IMPACT             3459\n",
       "DIVORCE            3426\n",
       "CRIME              3405\n",
       "MEDIA              2815\n",
       "WEIRD NEWS         2670\n",
       "GREEN              2622\n",
       "RELIGION           2556\n",
       "SCIENCE            2178\n",
       "WORLD NEWS         2177\n",
       "TECH               2082\n",
       "MONEY              1707\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "ENVIRONMENT        1323\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1129\n",
       "EDUCATION          1004\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          1612\n",
       "WELLNESS           887\n",
       "ENTERTAINMENT      790\n",
       "STYLE & BEAUTY     628\n",
       "PARENTING          601\n",
       "TRAVEL             487\n",
       "FOOD & DRINK       414\n",
       "HEALTHY LIVING     347\n",
       "QUEER VOICES       323\n",
       "BUSINESS           311\n",
       "WORLDPOST          287\n",
       "COMEDY             261\n",
       "SPORTS             260\n",
       "BLACK VOICES       238\n",
       "HOME & LIVING      210\n",
       "DIVORCE            186\n",
       "WEDDINGS           183\n",
       "CRIME              175\n",
       "IMPACT             175\n",
       "ARTS & CULTURE     174\n",
       "WOMEN              161\n",
       "MEDIA              136\n",
       "WEIRD NEWS         133\n",
       "GREEN              123\n",
       "TECH               117\n",
       "RELIGION           111\n",
       "WORLD NEWS         108\n",
       "SCIENCE            106\n",
       "MONEY               85\n",
       "GOOD NEWS           79\n",
       "FIFTY               68\n",
       "ENVIRONMENT         61\n",
       "COLLEGE             58\n",
       "EDUCATION           57\n",
       "LATINO VOICES       48\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "target = le.fit_transform(dataset['category'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WELLNESS', 'ENTERTAINMENT', 'BUSINESS', 'HEALTHY LIVING']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['category'].tolist()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31,  8,  2, 14])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = (dataset['headline'] + ' ' + dataset['short_description']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 Easy Guided Meditations For Relaxation The stress and strain of constantly being connected can sometimes take your life -- and your well-being -- off course. GPS',\n",
       " 'Corrupt Politician From \\'The Wire\\' Says Donald Trump Is Just Like Him \"Sheeeeeeeee-it!\"',\n",
       " \"Wells Fargo's New CEO Already Sounding A Little Stressed He hasn't even had the job for a week yet.\",\n",
       " 'Is It Possible To Outrun Your Own Fart? Today in things you never knew you needed to know..']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternAlphaNum = re.compile('[\\W_]+')\n",
    "stops = stopwords.words('english')\n",
    "wn = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# helper function to transform pos-tags\n",
    "# this is ridiculous btw\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def clean_text(text, lemma_or_stem='lemma', word_transformer=wn, pattern=patternAlphaNum, stops=stops):\n",
    "    # split to words and lowercase\n",
    "    words = [word.lower() for word in text.split()]\n",
    "    \n",
    "    # keep only alphanumeric characters\n",
    "    words_alphanum = [pattern.sub('', word) for word in words]\n",
    "    \n",
    "    # remove stopwords\n",
    "    words_notstop = [word for word in words_alphanum if not ((word in stops) or (word == ''))]\n",
    "    \n",
    "    # lemmatize using pos tags\n",
    "    if lemma_or_stem == \"lemma\":\n",
    "        pos_tags = [get_wordnet_pos(pos[1]) for pos in pos_tag(words_notstop)]\n",
    "        return ' '.join([word_transformer.lemmatize(word, pos=pos_tag) for word, pos_tag in zip(words_notstop, pos_tags)])\n",
    "    \n",
    "    # stemming\n",
    "    elif lemma_or_stem == \"stem\":\n",
    "        return ' '.join([word_transformer.stem(word) for word in words_notstop])\n",
    "    \n",
    "    # return raw words\n",
    "    else:\n",
    "        return ' '.join(words_notstop)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f1a8a05be14aada6ec2a5faf60385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca045e646e24ae786eff30ffbe5518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts_cleaned_lemma = [clean_text(text, 'lemma', wn) for text in tqdm(texts)]\n",
    "texts_cleaned_stem = [clean_text(text, 'stem', stemmer) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine them to 1 text\n",
    "all_cleaned_text_lemma = '\\n'.join(texts_cleaned_lemma)\n",
    "all_cleaned_text_stem = '\\n'.join(texts_cleaned_stem)\n",
    "\n",
    "# write to file\n",
    "with open('cleaned_text/cleaned_text_lemma.txt', 'wb') as f:\n",
    "    f.write(all_cleaned_text_lemma.encode('utf-8'))\n",
    "with open('cleaned_text/cleaned_text_stem.txt', 'wb') as f:\n",
    "    f.write(all_cleaned_text_stem.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "with open('cleaned_text/cleaned_text_stem.txt', 'rb') as f:\n",
    "    all_cleaned_text_stem = [line.decode('utf-8').strip('\\n') for line in f.readlines()]\n",
    "with open('cleaned_text/cleaned_text_lemma.txt', 'rb') as f:\n",
    "    all_cleaned_text_lemma = [line.decode('utf-8').strip('\\n') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "blobs = [TextBlob(text) for text in texts]\n",
    "polarities = [blob.polarity for blob in blobs]\n",
    "subjectivities = [blob.subjectivity for blob in blobs]\n",
    "lens = [len(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_types = ['unigram_min3','unibigram_min3', 'unibitrigram_min3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = {}\n",
    "tfidf['unigram_min3'] = TfidfVectorizer(min_df=3, max_df=0.5)\n",
    "tfidf['unibigram_min3'] = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.5)\n",
    "tfidf['unibitrigram_min3'] = TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectors = {\n",
    "    'lemma': {key: value.fit_transform(all_cleaned_text_lemma) for key, value in tfidf.items()},\n",
    "    'stem': {key: value.fit_transform(all_cleaned_text_stem) for key, value in tfidf.items()}\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_unigram_min3: (10000, 7249)\n",
      "lemma_unibigram_min3: (10000, 11094)\n",
      "lemma_unibitrigram_min3: (10000, 11478)\n",
      "stem_unigram_min3: (10000, 6705)\n",
      "stem_unibigram_min3: (10000, 10665)\n",
      "stem_unibitrigram_min3: (10000, 11062)\n"
     ]
    }
   ],
   "source": [
    "for key1 in text_vectors.keys():\n",
    "    for key2 in text_vectors[key1].keys():\n",
    "        print(f'{key1}_{key2}: {text_vectors[key1][key2].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE UNION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(key1, key2, target):\n",
    "    X = pd.DataFrame(text_vectors[key1][key2].todense())\n",
    "    X['polarity'] = polarities\n",
    "    X['subjectivity'] = subjectivities\n",
    "    X['lens'] = lens\n",
    "    \n",
    "    y = copy.copy(target)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE MODEL TO FIND REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem\n",
      "-------\n",
      "unigram_min3\n",
      "{'train': 0.41835820895522385, 'test': 0.3615151515151515}\n",
      "---\n",
      "unibigram_min3\n",
      "{'train': 0.43716417910447763, 'test': 0.36303030303030304}\n",
      "---\n",
      "unibitrigram_min3\n",
      "{'train': 0.4267164179104478, 'test': 0.3612121212121212}\n",
      "---\n",
      "lemma\n",
      "-------\n",
      "unigram_min3\n",
      "{'train': 0.41044776119402987, 'test': 0.353030303030303}\n",
      "---\n",
      "unibigram_min3\n",
      "{'train': 0.4, 'test': 0.3496969696969697}\n",
      "---\n",
      "unibitrigram_min3\n",
      "{'train': 0.43029850746268655, 'test': 0.36363636363636365}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for i in ['stem', 'lemma']:\n",
    "    print(i)\n",
    "    print('-------')\n",
    "    scores[i] = {}\n",
    "    for j in tfidf_types:\n",
    "        print(j)\n",
    "        tree = DTC(min_samples_split=0.1)\n",
    "        scores[i][j] = {}\n",
    "        X_train, X_test, y_train, y_test, le = create_dataset(i, j, target)\n",
    "        tree.fit(X_train, y_train)\n",
    "        scores[i][j]['train'] = tree.score(X_train, y_train)\n",
    "        scores[i][j]['test'] = tree.score(X_test, y_test)\n",
    "        del X_train, X_test, y_train, y_test, tree\n",
    "        gc.collect()\n",
    "        print(scores[i][j])\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's work with stemmed, uni and bigrams that appear on at least 3 texts and at most 50% of texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, le = create_dataset('stem', 'unibigram_min3', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to continue work later on\n",
    "X_train.to_csv('modeling_data/X_train.csv')\n",
    "X_test.to_csv('modeling_data/X_test.csv')\n",
    "pd.DataFrame(y_train).to_csv('modeling_data/y_train.csv')\n",
    "pd.DataFrame(y_test).to_csv('modeling_data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZE MODEL HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD MODEL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('modeling_data/X_train.csv')\n",
    "X_test = pd.read_csv('modeling_data/X_test.csv')\n",
    "y_train = pd.read_csv('modeling_data/y_train.csv')\n",
    "y_test = pd.read_csv('modeling_data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SET UP OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to optimize\n",
    "def rfccv(n_estimators, min_samples_split, max_features):\n",
    "    val = cross_val_score(\n",
    "        RFC(n_estimators=int(n_estimators),\n",
    "            min_samples_split=min_samples_split,\n",
    "            max_features=min(max_features, 0.999),\n",
    "            random_state=2\n",
    "        ),\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        scoring=None,\n",
    "        cv=3\n",
    "    ).mean()\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object which does optimization\n",
    "params_to_optimize = {'n_estimators': (10, 1000),\n",
    "                   'min_samples_split': (0.001, 0.2),\n",
    "                   'max_features': (0.1, 0.999)}\n",
    "\n",
    "rfcBO = BayesianOptimization(\n",
    "    rfccv,\n",
    "    params_to_optimize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log to file rather than prints\n",
    "logger_rfc = JSONLogger(path=\"logs/rfc_logs.json\")\n",
    "rfcBO.subscribe(Events.OPTMIZATION_STEP, logger_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform optimization\n",
    "rfcBO.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 53)\n",
    "print('Final Results')\n",
    "print('RFC: %f' % rfcBO.max['target'])\n",
    "\n",
    "print('-' * 53)\n",
    "print('Best Parameters')\n",
    "print(f'RFC: {rfcBO.max[\"params\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
